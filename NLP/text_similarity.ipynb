{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from scipy.spatial import distance\n",
    "import re\n",
    "from itertools import product\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(df[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jackard Similarity\n",
    "J = No of Common words / No of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(new, df):\n",
    "    len_ = []\n",
    "    for i in range(len(df)):\n",
    "        intersection = set(new).intersection(set(df[i]))\n",
    "        union = set(new).union(set(df[i]))\n",
    "        len_.append(len(intersection)/len(union))\n",
    "    return max(len_), df[len_.index(max(len_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity(sentence, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity\n",
    "\n",
    "Jaccard similarity takes only unique set of words for each sentence / document while cosine similarity takes total length of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(new, df):\n",
    "    X_list = word_tokenize(new)\n",
    "    len_ = []\n",
    "    for i in range(len(df)):\n",
    "        Y_list = word_tokenize(df[i]) \n",
    "        sw = stopwords.words('english')  \n",
    "        l1, l2 = [], [] \n",
    "        X_set = {w for w in X_list if not w in sw}  \n",
    "        Y_set = {w for w in Y_list if not w in sw} \n",
    "        rvector = X_set.union(Y_set)  \n",
    "        for w in rvector: \n",
    "            if w in X_set: l1.append(1) \n",
    "            else: l1.append(0) \n",
    "            if w in Y_set: l2.append(1) \n",
    "            else: l2.append(0) \n",
    "        c = 0\n",
    "        for i in range(len(rvector)): \n",
    "                c+= l1[i]*l2[i] \n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "        len_.append(cosine)\n",
    "    return max(len_), df[len_.index(max(len_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(sentence, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance_countvectorizer_method(s1, df):\n",
    "    len_ = []\n",
    "    for i in range(len(df)):\n",
    "        allsentences = [s1 , df[i]]\n",
    "        vectorizer = CountVectorizer()\n",
    "        all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n",
    "        text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n",
    "        text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n",
    "        cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n",
    "        len_.append(round((1-cosine)*100,2))\n",
    "    return max(len_), df[len_.index(max(len_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cosine_distance_countvectorizer_method(sentence, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloveFile = \"../data/glove.6B.50d.txt\"\n",
    "def loadGloveModel(gloveFile):\n",
    "    with open(gloveFile, encoding=\"utf8\" ) as f:\n",
    "        content = f.readlines()\n",
    "    model = {}\n",
    "    for line in content:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "model = loadGloveModel(gloveFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text):\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    words = letters_only_text.lower().split()\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    cleaned_words = list(set([w for w in words if w not in stopword_set]))\n",
    "    return cleaned_words\n",
    "\n",
    "def cosine_distance_between_two_words(word1, word2):\n",
    "    return (1- scipy.spatial.distance.cosine(model[word1], model[word2]))\n",
    "\n",
    "def cosine_distance_wordembedding_method(s1, s2):\n",
    "    len_ = []\n",
    "    vector_1 = np.mean([model[word] for word in preprocess(s1)],axis=0)\n",
    "    for i in range(len(s2)):\n",
    "        try:\n",
    "            vector_2 = np.mean([model[word] for word in preprocess(s2[i])],axis=0)\n",
    "            cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\n",
    "            len_.append(round((1-cosine)*100,2))\n",
    "        except:\n",
    "            pass\n",
    "    return max(len_), s2[len_.index(max(len_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distance_wordembedding_method(sentence,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ ~ ~ ~ ~ ~ ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Mover Distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When similar words are used, cosine similarity can be zero when it reality the text is similar, to avoid this WMD is used taking the word similarity in embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_fracdict(tokens):\n",
    "    cntdict = defaultdict(lambda : 0)\n",
    "    for token in tokens:\n",
    "        cntdict[token] += 1\n",
    "    totalcnt = sum(cntdict.values())\n",
    "    return {token: float(cnt)/totalcnt for token, cnt in cntdict.items()}\n",
    "\n",
    "def word_mover_distance_probspec(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=None):\n",
    "    all_tokens = list(set(first_sent_tokens+second_sent_tokens))\n",
    "    wordvecs = {token: wvmodel[token] for token in all_tokens}\n",
    "\n",
    "    first_sent_buckets = tokens_to_fracdict(first_sent_tokens)\n",
    "    second_sent_buckets = tokens_to_fracdict(second_sent_tokens)\n",
    "\n",
    "    T = pulp.LpVariable.dicts('T_matrix', list(product(all_tokens, all_tokens)), lowBound=0)\n",
    "\n",
    "    prob = pulp.LpProblem('WMD', sense=pulp.LpMinimize)\n",
    "    prob += pulp.lpSum([T[token1, token2]*euclidean(wordvecs[token1], wordvecs[token2])\n",
    "                        for token1, token2 in product(all_tokens, all_tokens)])\n",
    "    for token2 in second_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token1 in first_sent_buckets])==second_sent_buckets[token2]\n",
    "    for token1 in first_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token2 in second_sent_buckets])==first_sent_buckets[token1]\n",
    "\n",
    "    if lpFile!=None:\n",
    "        prob.writeLP(lpFile)\n",
    "\n",
    "    prob.solve()\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_mover_distance(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=None):\n",
    "    prob = word_mover_distance_probspec(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=lpFile)\n",
    "    return pulp.value(prob.objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = []\n",
    "for k in range(len(data)):\n",
    "    cur = data[k].lower().split()\n",
    "    for val in range(len(cur)):\n",
    "        if cur[val] not in toks:\n",
    "            toks.append(cur[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvmodel = gensim.models.Word2Vec([toks],min_count=1,size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text):\n",
    "#     letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    words = raw_text.lower().split()\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    cleaned_words = list(set([w for w in words if w not in stopword_set]))\n",
    "    return cleaned_words\n",
    "def find_wmd(s1, s2):\n",
    "    s1 = preprocess(s1)\n",
    "    len_ = []\n",
    "    for i in range(len(s2)):\n",
    "        \n",
    "        doc = preprocess(s2[i])\n",
    "        len_.append(word_mover_distance(s1, doc, wvmodel))\n",
    "    return max(len_), s2[len_.index(max(len_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_wmd(sentence, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ ~ ~ ~ ~ ~ ~ ~ ~ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InferSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InferSent is a sentence embeddings method that provides semantic sentence representations. It is trained on natural language inference data and generalizes well to many different tasks. \\\n",
    "<b>Original paper</b>: https://research.fb.com/wp-content/uploads/2017/09/emnlp2017.pdf \\\n",
    "<b>Architecture</b>: Attention based Ecoder Decoder Bi-LSTM \\\n",
    "<b>Codes</b>: https://github.com/facebookresearch/InferSent \\\n",
    "<b>Data</b>:The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment. (RTE)  \n",
    "\n",
    "\n",
    "(An <b>entailment</b> is a deduction or implication, that is, something that follows logically from or is implied by something else. In logic, an entailment is the relationship between sentences whereby one sentence will be true if all the others are also true.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import InferSent\n",
    "V = 1\n",
    "MODEL_PATH = '../models/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = '../data/glove.840B.300d.txt'\n",
    "infersent.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infersent.build_vocab(data, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = infersent.encode(data, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(no=3, dict_=sent):\n",
    "    sent_importance = {}\n",
    "    for i in range(len(y)):\n",
    "        if sent[i]=='<s>' or sent[i]=='</s>':\n",
    "            pass\n",
    "        else:\n",
    "            sent_importance.update({sent[i]: y[i]})\n",
    "    sort = sorted(sent_importance, key=sent_importance.get, reverse=True)\n",
    "    count = 0\n",
    "    for r in sort:\n",
    "        if count<no:\n",
    "            count += 1\n",
    "            print(r, sent_importance[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector, index, y, sent = infersent.visualize(sentence, tokenize=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
